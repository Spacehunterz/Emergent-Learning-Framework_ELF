[pytest]
# Pytest configuration for ELF Dashboard Backend Security Tests

# Test discovery patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test paths
testpaths = tests

# Console output options
addopts =
    -v
    --strict-markers
    --tb=short
    --disable-warnings
    --color=yes

# Coverage options (when using --cov)
# Example: pytest --cov=routers.auth --cov-report=html
[coverage:run]
source = routers,utils
omit =
    */tests/*
    */conftest.py
    */__pycache__/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False

# Coverage requirements
fail_under = 95

exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod

# Test markers
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, multi-component)
    security: Security tests (attack simulations)
    slow: Slow running tests (>1 second)
    redis: Tests requiring Redis
    performance: Performance benchmark tests

# Async support
asyncio_mode = auto

# Timeout for tests (prevent hanging)
timeout = 60
timeout_method = thread

# Logging
log_cli = False
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Warnings
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning

# Parallel execution (requires pytest-xdist)
# Uncomment to enable: pytest -n auto
# [pytest-xdist]
# numprocesses = auto
